코드잇 알고리즘 공부

탐색: 저장된 정보들 중에서 원하는 값을 찾는 것
-선형탐색알고리즘(linear search algorithm)
처음부터 쭉쭉쭉 살펴보다가 원하는 걸 찾으면 거기서 끝. 가장 오래 걸리는 경우 시간복잡도는 O(n)

-이진탐색알고리즘(binary search algorithm)
중간값 확인해서 반 버림. 남은 것 중에 중간값 확인하고 반 버림(sorted 일때만 가능하겠네) 1회 탐색마다 탐색범위가 절반으로 줄어들기 때문에 이진탐색이라고 부른다. 가장 오래 걸리는 경우 시간복잡도는 O(log(2)n) 메모장으로 어떻게 쓰냐.. 32개면 5번 걸린다는 얘기. 선형탐색보다 탐색 시간이 획기적으로 줄어드는 대신 정렬된 상태에서만 사용가능


정렬(sorting) : 원소들을 특정 순서로 정리하는 것
파이썬 리스트에는 sorted 함수나 sort 메소드가 있음
-선택정렬(selection sort)
전체탐색 후 가장 작은 값을 0번 인덱스에 위치 -> 또 전체탐색(0번 제외) 후 그 다음 작은 값을 1번 인덱스에 위치시킴 -> ... 반복

-삽입정렬(insertion sort) 
선택정렬이 각 위치에 어떤 값이 들어갈지를 찾는 방식이라면 얘는 각 값이 어떤 인덱스에 들어갈지 찾는다고 볼 수 있다. 0번 인덱스 확인 -> 1번까지 확인한 뒤 1번 인덱스가 0번보다 작다면 1번 인덱스 값을 0번에 삽입하고 0은 하나 뒤로 밀어줌 -> 2번까지 확인한 뒤 맞는 위치에 삽입 및 기존 애들을 밀어줌 -> 반복

https://www.toptal.com/developers/sorting-algorithms 여기 들어가면 다양한 sorting 방식을 직관적인 애니메이션으로 한번에 볼 수 있다. 너무 잘 만들었당

-시간복잡도를 '데이터가 많아질 수록 걸리는 시간이 얼마나 "급격히" 증가하는가' 라고 생각하니까 좀 더 이해가 쉽다. 미분값(경사도) 의 관점으로 보는거지


재귀함수(recursion)
-재귀함수에서는 부분문제를 생각해서 base case와 recursive case를 생각해줘야 한다. 코드로 너무 쉽게 옮기는 걸 보고 깜짝 놀랬다. 
-반복문으로 풀 수 있는 문제는 재귀함수로 풀 수 있다. 역도 마찬가지. 상황에 따라 더 깔끔하고 효율적인 걸 써주면 된다
-재귀함수를 사용하면 함수가 호출될 때 마다 call stack(이 함수가 끝나면 어디로 돌아갈지 알아야하니까 그 위치를 기록하는 것)이 쌓인다. 이게 너무 많이 쌓여서 프로그램이 중단되면 그게 stack overflow인거고. 파이썬은 내부적으로 call stack을 1000개로 제한하고 있으며 팩토리얼을 재귀함수로 구현했을 때 factorial(2000)을 하면 코드 자체는 문제가 없지만 call stack이 한계치를 넘어가기 때문에 recursion error가 발생한다. 


브루트포스(brute force)
직관적이고 명확함. 확실하게 답을 찾는 방법. 물론 인풋이 클 땐 안쓰는게 현명하겠죵



분할 정복(divide and conquer)
divide - conquer - combine
부분문제로 나눠서 해결하고 그 부분 해답들을 이용해서 원 문제를 푸는 것(기존 문제가 한번에 해결하기에 너무 어렵거나 크다고 판단될 때)
재귀 개념과 밀접하게 연결되어 있으며, conquer 단계에서 부분문제에 또 divide and conquer을 사용하기 때문에(문제가 충분히 작아져서 base case가 될 때까지) 알고리즘 중에서도 난이도가 높은 편

-merge sort
divide and conquer 을 통해 합병정렬(merge sort)을 할 수 있다. 리스트를 base case까지 분할한뒤 정렬해서 combine. 근데 매 단계 combine에서 두 리스트의 첫번째 값을 계속 비교해주면서 combine해야 하는데 이것 굳이 여러번 하는게 비효율적으로 보인다. 뭐가 좋은거지?

-quick sort
divide and conquer 을 이용한 또 다른 정렬방법 퀵정렬(quick sort)가 있다. merge sort를 해보면 알지만 사실상 combine 부분, 리스트의 값을 일일히 비교하면서 병합하는 과정이 제일 메인이고 복잡하다. 즉 divide와 conquer는 쉽고 combine이 복잡하다. quick sort는 반대로 divide가 중요하고 conquer과 combine은 간단하다. divide 단계에서 pivot 이라는 기준점을 하나 잡아서 걔를 기준으로 걔보다 작은 애들과 걔보다 큰 애들로 divide한다(이걸 partition 이라고 함. 즉 머지소트에서 merge 함수를 짜는게 메인이었던 것처럼 퀵소트는 partition이 그러함). 이런식으로 하면 combine 단계에서는 pivot을 기준으로 양 옆으로 붙여주면 되므로 일일히 비교하는 과정을 생략할 수 있다. 음 근데 그럼 퀵소트가 머지소트보다 훨씬 좋아보이는뎅... 머지소트는 언제 쓰는거지?

파티션과정에 대한 설명을 보니 small =[] big = []만들어서 for문 돌리면서 집어넣는게 아니라 in-place로 짠다. 흐음 멋지당 더 어려워졌지만 variable 생성없이 기존의 리스트를 [ -small-|pivot|-big- ] 으로 만드는게 참 깔끔하다. 머지소트는 새로운 리스트를 리턴하고 퀵소트는 파라미터 자체를 변경한다.



다이나믹 프로그래밍(Dynamic Programming)
이 알고리즘을 쓰기 위한 조건은 최적부분구조(optimal substructure)와 중복되는 부분문제(overlapping subproblems)이다.
1. 옵티멀 서브스트럭쳐. 
최적부분구조는 부분문제의 최적의답을 이용해서 답을 구할 수 있는 구조를 말한다. 앞서 풀었던 피보나치 수열을 구할 때 n-1과 n-2의 피보나치 값을 이용해 n 피보나치 값을 구할 수 있다. 이건 재귀구조지만 꼭 재귀가 아니어도 가능하다. a -> f 의 최단경로를 구하고 싶을 때 f로 향하는 노드가 c, d 라면, a -> c 최단경로와 a->d 최단경로를 이용해 에프까지의 최단 경로를 알아낼 수 있다. 피보나치처럼 두개를 더한 값이 곧 답이 되는 경우가 아니지만 얘네를 비교해서 최단경로를 찾아낼 수 있으니 이런 경우도 최적부분구조이다. (그럼 재귀구조는 전부 최적부분구조겠네)

2. 중복되는부분문제. 
문제를 부분문제로 나누기를 반복했을 때(base case 까지) 중복되는 부분 문제가 있는 경우를 말한다. 피보나치 수열 계산을 보면 fib(5)는 fib(4)와 fib(3)으로 나뉘는데 fib(4)를 계산하기 위해서는 다시 fib(3)과 fib(2)로 나누게 된다. 즉 fib(3)이 fib(3)쪽에서도 연산이 되고 fib(4)쪽에서도 연산이 된다. 이걸 양쪽에서 각각 계산하고 있으면 너무 비효율적이겠지? 이런 경우를 말한다. // 부분문제로 나눌수 있음 != 중복되는부분문제임 이다. 즉 부분문제로 나눌 수 있다고해서 그런 케이스들이 전부 중복되는 부분문제를 가지는게 아니다. 머지 소트를 보면 리스트를 반으로 나눴을 때 양쪽 리스트는 아예 다르고 양쪽에서 파생되는 부분문제들도 전혀 겹침없이 독립적이다. 이런 경우는 overlapping subproblems 가 아니다

부분문제로 나눌 수 있고(최적 부분 구조가 있고), 중복되는 부분문제들이 있을 경우 다이나믹 프로그래밍을 쓸 수 있다. 다이나믹 프로그래밍이 지향하는 바는 같은 계산을 여러번하지 말고 기억해뒀다가 다시 사용하자는 것. 그러니까 중복되는 부분 문제들을 좀 더 효율적으로 처리하자는 것이다. 

다이나믹 프로그래밍을 구현하는 방식은 또 두가지로 나뉜다: memoization, tabulation (둘 다 생전 처음 들어봐..)
-메모이제이션
메모메모를 해두는 것. 요 부분문제가 다른 데서 또 등장할 수도 있으니 연산한 값을 캐시에 저장해놓고 다음에 만나면 연산하는 대신 캐시에서 값을 가져다 쓰는 방식이다. 맨위(최종목표)에서부터 하나씩 내려가기 때문에 top-down approach라고 할 수 있다. (근데 결국은 base case까지 내려가서 다시 계산하면서 올라오는 과정에 캐시를 기록하는거 아녀?)

-타뷸레이션
fib(n)을 구하기 위해 fib(1)을 기록하고, fib(2)를 기록하고 ... fib(n-1)까지 기록하면서 올라가는 방식. bottom up approach 다. 표(table)를 채워나가는 과정처럼 생각할 수 있으므로 tabulation이라고 한다. 메모이제이션이 재귀를 사용한다면 얘는 for문을 통해 작성할 수 있다.

-메모이제이션은 재귀 / 타뷸레이션은 for문  - 재귀는 stack overflow가 일어날 가능성이 있음. 타뷸레이션은 필요하지 않은 부분문제들을 추가로 연산할 가능성이 있음. 요구한 걸 계산하는게 아니라 base case부터 전부 계산해놓는 거니까(eager evaluation). 

-결국 다이나믹 프로그래밍은 공간을 좀 더 써서 시간을 획기적으로 줄일 수 있는 방법. 시간복잡도와 공간복잡도 모두 O(n)이다. 조금 더 신경을 쓰면 공간복잡도를 O(1)로 만들수도 있다. 피보나치의 경우 사실 상 n-1과 n-2만 있으면 n을 구할 수 있으니, table에 전부 저장하는 대신 prev와 current를 계속 업데이트 해주는 식으로 진행하면 O(1)에 가능하다! 물론 문제에 따라 이게 안되는 경우도 있다. 그리고 메모이제이션은 이렇게 안될 거 같은뎅.. 나중에 좀 더 찾아보기


그리디 알고리즘(greedy algorithm)
내가 아는 그 greedy가 맞다. 즉 지금 눈앞에서의 최선의 선택을 내리는 것. 이런 경우 local optimum에 도달할 위험이 있다. 그리디 알고리즘은 빠르고 간단하기 때문에 다른 알고리즘이 느려서 도저히 쓸 수 없을 경우 사용할 수 있다. 또한 꼭 글로벌 옵티멈에 도달하지 않아도 괜찮은 경우, 혹은 그리디 알고리즘을 써도 글로벌 옵티멈에 도달하는 경우(아마 convex 해서 로컬옵티멈이 없는 경우일 듯? 머신러닝의 GD개념과 유사하다) 등에 사용한다

그리디 알고리즘을 통해 도출한 결과값이 최적의 값인 문제인지를 판별하기 위해서는 두가지 조건을 따져보면 된다. 
1. 옵티멀 서브스트럭처
- 위의 다이나믹 프로그래밍에서 배운 그 개념이다.
2. 탐욕적 선택 속성(greedy choice property)
- 각 부분문제에서의 greedy choice가 전체 문제의 최적값으로 이어진다면 탐욕적 선택 속성을 가지고 있다고 말한다. 말이 쉽지, 이걸 따지는게 어려울 것 같다. 예를 들어 최대한 적은 동전을 사용해서 돈을 거슬러주는 방법을 따지고 싶다고 하자.(동전 종류는 500/100/50/10) 이 때 매 단계에서 가능한 가장 큰 동전을 고르는 것이 곧 전체 문제의 최적의 답으로 이어진다. 500원을 선택할 수 있는 상황에서 100원을 고르면 4개나 더 필요함을 우리는 알고있으니까. 이런 경우가 탐욕적 선택 속성을 가지고 있는 경우다. 근데 이건 상식적으로 우리가 다 동의하는거고 문제가 더 어려워지면 이 속성 여부를 어떻게 증명할 수 있을지 흠. (추가로 이 문제는 1번 조건도 만족한다. 600원일 경우 500한개+100원 / 100한개+400원 / 50한개+450원 / 10한개+490원 으로 부분문제를 나눌 수 있으니까)

문제가 이 두가지 속성을 지니고 있다면, 그리디 알고리즘을 통해 최적의 값을 찾아낼 수 있다(로컬 옵티멈에 도달할 위험이 없다).

오호 위의 예시에서 만약 동전종류가 100원 70원 10원일 경우를 보자. 나는 안일하게 100원 = 70원 + 10원*3 이니까 이것도 그리디 초이스 프로퍼티라고 생각했다. 근데 140원일 경우 100원은 100원*1 + 10원*4고 70원은 70원*2 이네.. 따라서 이 케이스는 탐욕적 선택 속성이 없다. 